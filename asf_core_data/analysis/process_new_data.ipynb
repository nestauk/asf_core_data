{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for producing processed EPC and MCS data and merging them into one table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the asf-core-data repo for the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-28 11:56:52,876 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "from asf_core_data.getters.data_getters import download_core_data\n",
    "from asf_core_data import generate_and_save_mcs\n",
    "from asf_core_data import load_preprocessed_epc_data\n",
    "from asf_core_data.getters.epc import data_batches\n",
    "from asf_core_data.pipeline.preprocessing import preprocess_epc_data, data_cleaning\n",
    "from asf_core_data.pipeline.data_joining import merge_install_dates, merge_proc_datasets\n",
    "from asf_core_data.getters import data_getters\n",
    "from asf_core_data.config import base_config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing EPC\n",
    "\n",
    "Currently, we're still handling the EPC processing by downloading and processing it locally. In the future, this will be done directly via S3. \n",
    "For now, we need to download the raw EPC data into our local data foler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DATA_DIR = '/path/to/dir'\n",
    "\n",
    "if not os.path.exists(LOCAL_DATA_DIR):\n",
    "    os.makedirs(LOCAL_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_core_data('epc_raw', LOCAL_DATA_DIR, batch='newest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local input dir\n",
      "---------------\n",
      "Available batches: ['2022_Q3_complete']\n",
      "Newest batch: 2022_Q3_complete\n"
     ]
    }
   ],
   "source": [
    "# Check whether newest batch shows up a newest in local data dir\n",
    "print(\"Local input dir\\n---------------\")\n",
    "print(\"Available batches:\", data_batches.get_all_batch_names(data_path=LOCAL_DATA_DIR, check_folder='inputs'))\n",
    "print(\"Newest batch:\", data_batches.get_most_recent_batch(data_path=LOCAL_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw data to /Users/juliasuter/Documents/My_ASF_data/outputs/EPC/preprocessed_data/2022_Q3_complete/EPC_GB_raw.csv\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-37cc5c5335e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Process new batch of EPC data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m epc_full = preprocess_epc_data.load_and_preprocess_epc_data(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_DATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"newest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreload_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/Documents/repositories/asf_core_data/asf_core_data/pipeline/preprocessing/preprocess_epc_data.py\u001b[0m in \u001b[0;36mload_and_preprocess_epc_data\u001b[0;34m(data_path, rel_data_path, subset, usecols, batch, n_samples, remove_duplicates, save_data, reload_raw)\u001b[0m\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     epc_df = preprocess_data(\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mepc_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/repositories/asf_core_data/asf_core_data/pipeline/preprocessing/preprocess_epc_data.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(df, remove_duplicates, data_path, subset, batch, save_data, verbose)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Save unaltered_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprocessing_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3549\u001b[0m         )\n\u001b[1;32m   3550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3551\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         )\n\u001b[0;32m-> 1180\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/asf_core_data/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_native_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/writers.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process new batch of EPC data\n",
    "epc_full = preprocess_epc_data.load_and_preprocess_epc_data(\n",
    "    data_path=LOCAL_DATA_DIR, batch=\"newest\", subset='GB',\n",
    "    reload_raw=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing MCS\n",
    "\n",
    "After processing the EPC data, it has to be uploaded to S3 again for further processing. In the future, this will happen automatically.\n",
    "In order for the following code to work, you should at least upload the following file to the S3 asf-core-data bucket: `LOCAL_DATA_DIR/BATCH_NAME/EPC_GB_preprocessed.csv`\n",
    "\n",
    "You can do this using a command as the following in your terminal:\n",
    "\n",
    "`aws s3 cp EPC_GB_preprocessed.csv s3://asf-core-data/outputs/EPC/preprocessed_data/2022_Q3_complete/`\n",
    "\n",
    "\n",
    "**Note:**\n",
    "An additional step will be added here or included in `generate_and_save_mcs()`. We will need to process the MCS historical installer data and add the unique installation ID to the MCS installations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to process MCS data and join it with EPC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installations files\n",
      "inputs/MCS/latest_raw_data/mcs_installations_2021.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installations_2022_q1.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installations_2022_q2.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installations_2022_q3.xlsx\n",
      "\n",
      "Installer files\n",
      "inputs/MCS/latest_raw_data/mcs_installations_2022_q1.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installer_information_2022_04_06.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installer_information_2022_07_25.xlsx\n",
      "inputs/MCS/latest_raw_data/mcs_installers.xlsx\n",
      "Number of records before removing duplicates: 170275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/mcs/generate_mcs_data.py:136: UserWarning: Not all installation file columns are the same.\n",
      "  concat_installations = pd.concat(installations_dfs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after removing duplicates: 170237\n",
      "Shape of loaded data: (168574, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/mcs/process/process_mcs_installations.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hps[\"cluster\"].loc[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in S3: /outputs/MCS/mcs_installations_230228.csv\n",
      "Getting EPC data...\n",
      "Forming a matching...\n",
      "- Forming an index...\n",
      "- Forming a comparison...\n",
      "- Computing a matching...\n",
      "Joining the data...\n",
      "After joining:\n",
      "-----------------\n",
      "Total records: 267336\n",
      "Number matched with EPC: 238510\n",
      "\n",
      "\n",
      "Saved in S3: /outputs/MCS/mcs_installations_epc_full_230228.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/mcs/process/mcs_epc_joining.py:385: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  joined_df[\"last_epc_before_mcs\"].iloc[last_epc_before_mcs_indices] = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved in S3: /outputs/MCS/mcs_installations_epc_most_relevant_230228.csv\n"
     ]
    }
   ],
   "source": [
    "# Get MCS and join with MCS\n",
    "generate_and_save_mcs(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the EPC and MCS\n",
    "\n",
    "Finally, we load the EPC data and merge it with the MCS installations data for computing the best approximation for a heat pump installation date. You can also load the data from 'S3' insteadl of the local data dir, but if you have it downloaded it's faster.\n",
    "\n",
    "All these steps are summarised in the function `merging_pipeline()` in `merge_proc_datasets.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed EPC data \n",
    "prep_epc = load_preprocessed_epc_data(data_path=LOCAL_DATA_DIR, version='preprocessed',\n",
    "                                       #usecols=['UPRN', 'INSPECTION_DATE', 'HP_INSTALLED', 'HP_TYPE'],  # use fewer fields for testing to save time\n",
    "                                       batch='newest'\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-28 02:11:48,413 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"FIRST_HP_MENTION\"] = df[identifier].map(dict(first_hp_mention))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240089, 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"MCS_AVAILABLE\"] = ~df[\"HP_INSTALL_DATE\"].isna()\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HAS_HP_AT_SOME_POINT\"] = ~df[\"FIRST_HP_MENTION\"].isna()\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ARTIFICIALLY_DUPL\"] = False\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALL_DATE\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:219: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"EPC HP entry before MCS\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALLED\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:227: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALL_DATE\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"No EPC HP entry after MCS\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:243: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALLED\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:246: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALL_DATE\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:252: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALLED\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:274: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALLED\"] = np.where(\n",
      "/Users/juliasuter/Documents/repositories/asf_core_data/asf_core_data/pipeline/data_joining/merge_install_dates.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"HP_INSTALL_DATE\"] = np.where(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19047896, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add more precise estimations for heat pump installation dates via MCS data\n",
    "epc_with_MCS_dates = merge_install_dates.manage_hp_install_dates(\n",
    "    prep_epc\n",
    ")\n",
    "\n",
    "epc_with_MCS_dates.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EPC data with enhanced installation dates can then be merged with MCS installation data. This will standardise features such as HP_INSTALLED and HP_TYPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epc_with_MCS_dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0fd2012f3a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepc_mcs_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_proc_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_proc_epc_and_mcs_installations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepc_with_MCS_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mepc_mcs_processed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epc_with_MCS_dates' is not defined"
     ]
    }
   ],
   "source": [
    "epc_mcs_processed = merge_proc_datasets.merge_proc_epc_and_mcs_installations(epc_with_MCS_dates, verbose=True)\n",
    "epc_mcs_processed.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get historical installer data (and finally merge it with the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/MCS/installers/mcs_historical_installers_20230207.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_unique_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>mcs_certificate_number</th>\n",
       "      <th>certification_body</th>\n",
       "      <th>address_1</th>\n",
       "      <th>address_2</th>\n",
       "      <th>town</th>\n",
       "      <th>county</th>\n",
       "      <th>postcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>solar_pv_certified</th>\n",
       "      <th>wind_turbine_certified</th>\n",
       "      <th>solar_thermal_certified</th>\n",
       "      <th>battery_storage_certified</th>\n",
       "      <th>air_source_hp_certified</th>\n",
       "      <th>ground_water_source_hp_certified</th>\n",
       "      <th>hot_water_hp_certified</th>\n",
       "      <th>exhaust_air_hp_certified</th>\n",
       "      <th>gas_absorbtion_hp_certified</th>\n",
       "      <th>solar_assisted_hp_certified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t j galvin plumbing heating engineers</td>\n",
       "      <td>T J Galvin Plumbing &amp; Heating Engineers</td>\n",
       "      <td>1283</td>\n",
       "      <td>MCS</td>\n",
       "      <td>Brandoak House</td>\n",
       "      <td>Stone</td>\n",
       "      <td>Berkeley</td>\n",
       "      <td>Gloucestershire</td>\n",
       "      <td>GL139LA</td>\n",
       "      <td>51.652315</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paragon systems scotland</td>\n",
       "      <td>Paragon Systems (Scotland) Ltd</td>\n",
       "      <td>1286</td>\n",
       "      <td>MCS</td>\n",
       "      <td>The Office, Corbie Cottage</td>\n",
       "      <td>Maryculter</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Aberdeenshire</td>\n",
       "      <td>AB125FT</td>\n",
       "      <td>57.089012</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carillion energy services</td>\n",
       "      <td>Carillion Energy Services Limited</td>\n",
       "      <td>1290</td>\n",
       "      <td>MCS</td>\n",
       "      <td>Partnership House, Regent Farm Road</td>\n",
       "      <td>Gosforth</td>\n",
       "      <td>Newcastle Upon Tyne</td>\n",
       "      <td>Tyne and Wear</td>\n",
       "      <td>NE33AF</td>\n",
       "      <td>55.010499</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>edwards uk</td>\n",
       "      <td>Edwards UK Ltd t/a Nugenn</td>\n",
       "      <td>1292</td>\n",
       "      <td>MCS</td>\n",
       "      <td>Suite 2, Cumbria House</td>\n",
       "      <td>Gillwilly Road</td>\n",
       "      <td>Penrith</td>\n",
       "      <td>Cumbria</td>\n",
       "      <td>CA119FF</td>\n",
       "      <td>54.665127</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jdk enterprises</td>\n",
       "      <td>JDK Enterprises Ltd t/a Solar Air UK</td>\n",
       "      <td>1294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 Hilltop</td>\n",
       "      <td>Stanley Road</td>\n",
       "      <td>Whitstable</td>\n",
       "      <td>Kent</td>\n",
       "      <td>CT54QE</td>\n",
       "      <td>51.346942</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       company_unique_id  \\\n",
       "0  t j galvin plumbing heating engineers   \n",
       "1               paragon systems scotland   \n",
       "2              carillion energy services   \n",
       "3                             edwards uk   \n",
       "4                        jdk enterprises   \n",
       "\n",
       "                              company_name  mcs_certificate_number  \\\n",
       "0  T J Galvin Plumbing & Heating Engineers                    1283   \n",
       "1           Paragon Systems (Scotland) Ltd                    1286   \n",
       "2        Carillion Energy Services Limited                    1290   \n",
       "3                Edwards UK Ltd t/a Nugenn                    1292   \n",
       "4     JDK Enterprises Ltd t/a Solar Air UK                    1294   \n",
       "\n",
       "  certification_body                            address_1       address_2  \\\n",
       "0                MCS                       Brandoak House           Stone   \n",
       "1                MCS           The Office, Corbie Cottage      Maryculter   \n",
       "2                MCS  Partnership House, Regent Farm Road        Gosforth   \n",
       "3                MCS               Suite 2, Cumbria House  Gillwilly Road   \n",
       "4                NaN                            6 Hilltop    Stanley Road   \n",
       "\n",
       "                  town           county postcode   latitude  ...  \\\n",
       "0             Berkeley  Gloucestershire  GL139LA  51.652315  ...   \n",
       "1             Aberdeen    Aberdeenshire  AB125FT  57.089012  ...   \n",
       "2  Newcastle Upon Tyne    Tyne and Wear   NE33AF  55.010499  ...   \n",
       "3              Penrith          Cumbria  CA119FF  54.665127  ...   \n",
       "4           Whitstable             Kent   CT54QE  51.346942  ...   \n",
       "\n",
       "   solar_pv_certified wind_turbine_certified  solar_thermal_certified  \\\n",
       "0               False                  False                    False   \n",
       "1               False                  False                    False   \n",
       "2                True                  False                     True   \n",
       "3               False                  False                    False   \n",
       "4               False                  False                     True   \n",
       "\n",
       "  battery_storage_certified air_source_hp_certified  \\\n",
       "0                     False                    True   \n",
       "1                     False                    True   \n",
       "2                     False                    True   \n",
       "3                     False                    True   \n",
       "4                     False                    True   \n",
       "\n",
       "  ground_water_source_hp_certified hot_water_hp_certified  \\\n",
       "0                             True                  False   \n",
       "1                             True                  False   \n",
       "2                             True                  False   \n",
       "3                             True                  False   \n",
       "4                             True                  False   \n",
       "\n",
       "  exhaust_air_hp_certified  gas_absorbtion_hp_certified  \\\n",
       "0                    False                        False   \n",
       "1                     True                        False   \n",
       "2                     True                        False   \n",
       "3                     True                        False   \n",
       "4                     True                        False   \n",
       "\n",
       "  solar_assisted_hp_certified  \n",
       "0                       False  \n",
       "1                       False  \n",
       "2                       False  \n",
       "3                       False  \n",
       "4                       False  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest_hist_inst_batch = data_batches.get_latest_hist_installers()\n",
    "\n",
    "print(newest_hist_inst_batch)\n",
    "\n",
    "# # Load MCS\n",
    "mcs_inst_data = data_getters.load_s3_data(\n",
    "    base_config.BUCKET_NAME,\n",
    "    newest_hist_inst_batch,\n",
    ")\n",
    "mcs_inst_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('asf_core_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "151bc73aaa639fad610cb4b2d60afec3c77157d89b4c3022086af37771d09181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
